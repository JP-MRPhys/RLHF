import torch
from actor import actor
from reward import reward_model
from einops import rearrange

import config

class ActorCritic(tornh.nn.Module):

    def __init__(self, config) -> None:
        super().__init__()

        self.actor=actor(config.actor)
        self.critic=reward_model(config.actor)
        self.debug=config.actor.debug



    def forward(
        self,
        sequences: torch.Tensor,
        sequences_mask: torch.Tensor,
        action_len: int,
    ) -> Tuple:
        """Given the whole sequences, use the actor forward to get the logits
            for each token in the sequence and the critic forward to get the
            values for each generation step.
        Args:
            sequences (torch.Tensor): Sequences composed of [states, actions]
            sequence_mask (torch.Tensor): Mask for the sequences
            action_length (int): Length of the actions in the sequences
        Returns:
            action_logits (torch.Tensor): Logits for the actions in the
                sequences
            values (torch.Tensor): Values for the actions in the sequences
        """
        # use a single forward on the whole sequence
        # to get pi(y | x) and ignore predicted output
        actions_logits = self.actor.forward(sequences, sequences_mask)
        values = self.critic.forward(sequences, sequences_mask)

        # return only logits and values for the actions taken
        real_actions_logits = actions_logits[:, -action_len:, :]
        real_values = values[:, -action_len:]

        if self.debug:
            print("ActorCritic.forward")
            print("action_len", action_len)
            print("sequences.shape", sequences.shape)
            print("sequences", sequences)
            print("real_action_logits.shape", actions_logits.shape)
            print("real_action_logits", actions_logits)
            print("real_values.shape", values.shape)
            print("real_values", values)

        return (
            real_actions_logits,
            real_values,
        )

    @torch.no_grad()
    def generate(
        self, states: torch.Tensor, state_mask: torch.Tensor
    ) -> Tuple:
        """Generate actions, actions_logits, values and sequences from states
        Args:
            states (torch.Tensor): user inputs
            state_mask (torch.Tensor): Mask for the states of the environment
        Returns:
            actions (torch.Tensor): Actions generated from the states
            actions_logits (torch.Tensor): Logits for the actions generated
                from the states (i.e. pi(y | x))
            values (torch.Tensor): Values generated by the critic model
                for the actions generated by the actor (i.e. V(x))
            sequences (torch.Tensor): Sequences generated from the states
                as [states, actions]
        """
        # generate action sequence
        actions, sequence = self.actor.generate(states, state_mask)
        sequences_mask = sequence != self.actor.tokenizer.pad_token_id
        action_len = actions.shape[1]

        # generate actions_logits and values
        actions_logits, values = self.forward(
            sequence, sequences_mask, action_len
        )
        if self.debug:
            print("ActorCritic.generate")
            print("actions shape", actions.shape)
            print("actions", actions)
            print("sequence shape", sequence.shape)
            print("sequence", sequence)
            print("actions_logits shape", actions_logits.shape)
            print("actions_logits", actions_logits)
            print("values shape", values.shape)
            print("values", values)

        return actions, actions_logits, values, sequence, sequences_mask


class RL_trainer():
    def __init__(self, config) -> None:
        pass


        self.config=config.actor.debug
        self.actorcritic=ActorCritic(config)

        self.actor_optmizer=torch.optim.Adam(self.actorcritic.actor.parameters, lf=config.actor.lr)
        self.critic_optmizer=torch.optim.Adam(self.actorcritic.critic.parameters, lf=config.critic.lr)

        self.reward=reward_model(config.reward)

        self.eps=1e-8

    def save(self):
        return
    
    def load(self):
        return
    
    def learn(self):

               # get parameters
        epochs = self.config.trainer.epochs
        actor_eps_clip = self.config.trainer.actor_eps_clip
        critic_eps_clip = self.config.trainer.critic_eps_clip
        beta_s = self.config.trainer.beta_s
        batch_size = self.config.trainer.batch_size
        device = self.config.trainer.device
        data=[]
        self.actorcritic.train()

        for epoch in range(epochs):

            for i , (state, old_action, sequence, old_values, rewards, old_actions_log_probs, sequence_mask,) in enumerate(data):


                rewards = rearrange(rewards, "b -> b 1")

                actions_len=old_action.shape[-1]
                actions_logits, values=self.actorcritic.forward(sequence,sequence_mask,actions_len)
                action_prob=torch.softmax(actions_logits, dim=-1).max(dim=-1)/values

                actions_log_prob=torch.log(action_prob*self.eps)

                entropies=(action_prob*actions_log_prob).sum(dim=-1)  

                 # compute KL divergence
                kl_div_loss = (
                    (action_prob * (old_actions_log_probs - actions_log_prob))
                    .sum(dim=-1)
                    .mean()
                )


                # compute PPO Loss -- Whan dimensions are different
                # (especially the values and the probs are
                #  multiplied directly with the reward)
                ratios = (actions_log_prob - old_actions_log_probs).exp()
                advantages = rewards - old_values
                # normalize advantages
                advantages = (advantages - advantages.mean(dim=-1)) / (
                    advantages.std() + self.eps
                )
                surr1 = advantages * ratios
                surr2 = (
                    torch.clamp(ratios, 1 - actor_eps_clip, 1 + actor_eps_clip)
                    * advantages
                )
                policy_loss = -torch.min(surr1, surr2) - beta_s * entropies
                policy_loss = policy_loss.mean()
                loss = policy_loss + kl_div_loss
                # check if loss item is nan
                if torch.isnan(loss):
                    raise ValueError("Loss is nan")
                print("loss", loss.item())


                               # update actor with loss
                self.actor_optim.zero_grad()
                loss.backward()
                self.actor_optim.step()

                torch.cuda.synchronize(device)

                # compute value loss
                value_loss_clipped = old_values + (values - old_values).clamp(
                    -critic_eps_clip, critic_eps_clip
                )
                value_loss1 = (value_loss_clipped - rewards) ** 2
                value_loss2 = (values - rewards) ** 2
                value_loss = torch.max(value_loss1, value_loss2).mean()
                if torch.isnan(value_loss):
                    raise ValueError("Value loss is nan")
                print("value_loss", value_loss.item())

                # upate critic with loss
                self.critic_optim.zero_grad()
                value_loss.backward()
                self.critic_optim.step() 

        self.actorcritic.eval()
        print("Completed training actor and critic")


    def train(self) ->None:

        num_episodes = self.config.trainer.num_episodes
        max_timesteps = self.config.trainer.max_timesteps
        num_examples = self.config.trainer.num_examples
        update_timesteps = self.config.trainer.update_timesteps
        batch_size = self.config.trainer.batch_size
        update_checkpoint = self.config.trainer.update_checkpoint
        device = self.config.trainer.device

        print("Start RL Training")
        # check dimensions consistency
        # at each time step num_examples memories are generated
        number_of_memories_per_learn_iteration = (
            num_examples * update_timesteps
        )
        # the number of memories must be a multiple of the batch size
        assert (
            number_of_memories_per_learn_iteration % batch_size == 0
        ), "The number of memories must be a multiple of the batch size"
        # the total number of timesteps is
        total_number_of_timesteps = num_episodes * max_timesteps
        # the update_timesteps must be a multiple
        #  of the total number of timesteps
        assert total_number_of_timesteps % update_timesteps == 0, (
            "The number of timesteps (num_episodes*max_timesteps)"
            "must be a multiple of the update_timesteps"
        )      